\chapter{INTRODUCTION}

\section{Origins of Computational Chemistry}
The seeds of computational chemistry were sown in the mid-1800s with Ludwig
Eduard Boltzmann's formulation of \textit{statistical mechanics}. In an era when
the existence of atoms and molecules was hotly disputed within the physics
community, Boltzmann fathered a theory in which the behavior and interaction of
individual atoms or molecules on the microscopic scale could be used to
describe and predict macroscopic phenomena. Using his theorems and equations,
it became possible to reduce the problem of simulating $\sim 10^{23}$ molecules
to simulating $\sim 1$ molecule. Boltzmann used this to great effect in
describing and deriving previously-known, phenomenological equations for ideal
gases, such as the widely known equation of state, $P V = n R T$. All that
remains to provide the foundation for using molecular simulations is the proper
description of atoms and molecules on the microscopic scale.

The theories required to accurately model the behavior of individual atoms
interacting with each other and their surroundings would not be developed until
the first half of the $20^{th}$ century with the advent of \textit{quantum
mechanics}. The limits of classical mechanics became apparent when considering
the Rayleigh-Jeans formula for calculating the spectral emission of a radiating
black body. The Rayleigh-Jeans law, given by Eq. \ref{eq1:RayleighJeans} and
completely derived from the laws of classical mechanics, predicts infinite
emission at high frequencies---a clear violation of the well-established law of
conservation of energy that directly contradicts experimental observations.
\begin{equation}
   B_{\nu} (T)  = \frac{2 \nu^2 k T} {c^2}
   \label{eq1:RayleighJeans}
\end{equation}
To address this apparent disparity, Max Planck suggested that the error in the
classical mechanical approach was to assume a continuous emission spectrum.
Instead, Planck suggested that the emission spectra was quantized, leading to an
equation that agreed much closer with experiment. This idea of quantized energy
emissions, while developed to reconcile the mathematics of black-body radiation
with experimental measurements, would forever change our understanding of the
microscopic world.

As quantum mechanics matured, our ability to explain and predict behavior at the
atomic scale dramatically improved. In 1929, Paul Dirac proclaimed, ``The
fundamental laws necessary for the mathematical treatment of a large part of
physics and the whole of chemistry are thus completely known, and the difficulty
lies only in the fact that the application of these laws leads to equations that
are too complex to be solved.'' Even approximations designed to simplify the
equations of quantum mechanics in molecular systems resulted in computations too
complex to apply to all but the simplest systems. With the fundamental theory
necessary to describe single molecules and the machinery required to extend
that description to experimental measurements now established, computers
provided the catalyst that thrust theoretical chemistry into a prominent role in
the field.

The next sections will describe the theory of quantum mechanics and the
approximations typically employed to simplify its equations, followed by
a description of statistical mechanics.

\subsection{Quantum Mechanics}

Twenty years after Planck introduced the idea of quantized oscillators to
explain black-body radiation, Erwin Schr\"odinger introduced a wave equation
formulation of quantum mechanics (QM). \cite{Schrodinger1926} Schr\"odinger's
equation (Eq. \ref{eq1:SchrodingerEquation}) bears a strong resemblance to
Hamilton's formulation of classical mechanics by employing an analogous
\textit{Hamiltonian} operator comprised of a kinetic energy term (related to the
momentum operator) and a potential energy term.
\begin{align}
   E \Psi(\vec{x}, t) & = \hat{H} \Psi(\vec{x}) \nonumber \\
   & = \left ( -\frac {\hbar ^ 2} {2 m} \bigtriangledown ^ 2 + V(\vec{x})
   \right) \Psi(\vec{x})
   \label{eq1:SchrodingerEquation}
\end{align}
where $E$ is the total energy, $\hat{H}$ is the Hamiltonian operator, and
$\Psi(\vec{x}, t)$ is the wavefunction---the central object of Schr\"odinger's
equation containing all of the information and properties inherent to the
system.

Equation \ref{eq1:SchrodingerEquation} is a special form of Schr\"odinger's
equation corresponding to a stationary state (\ie the potential function is
time-independent, so the energy for that state is constant). In chemistry when
we wish to calculate observable properties of a system composed of atoms, the
kinetic energy is the sum of the kinetic energies of the atomic particles in the
system, and the potential energy is calculated as the interaction of all charged
particles---protons and electrons---in the electric field they create.

The wavefunction contains all of the information about each of the particles in
the system. As the number of particles in the system increases, so too does the
complexity of wavefunction and the effort required to solve Eq.
\ref{eq1:SchrodingerEquation}. Therefore, we turn to a number of approximations
developed to simplify computing a solution to Schr\"odinger's equation.

\subsubsection{Born-Oppenheimer Approximation}

The Born-Oppenheimer approximation (BOA) is almost ubiquitous in the field.
Electrons can move far more rapidly than nucleons since electrons are roughly
\mbox{1 000} times lighter. This implies that electrons can reorganize around
moving nuclei so quickly that nuclear protons are always subject to the
potential from the average electric field generated by the electrons.

Using the BOA, the wavefunction of a molecular system can be separated into two
parts: an electronic part where the nuclei are treated as fixed point charges,
and a nuclear part where the electrons are treated as a mean field.
\cite{McQuarrie_Book_PhysChem_1997} So critical is the BOA to computational
chemistry that it appears at the heart of nearly every aspect of the field.

\subsubsection{Computational Quantum Mechanics}
\label{sec:CompQuantumMech}

The main goal of most QM calculations in chemistry and molecular physics is to
determine atomic and molecular properties of the system by estimating the
electronic part of the wavefunction from the BOA. These calculations have
provided valuable assistance to experimental investigations. QM calculations
can provide reliable measurements of molecular geometries,
\cite{Jeletic_JOrganometChem_2011_v696_p3127} potential and free energy barriers
of chemical reactions, \cite{Chandrasekhar_JAmChemSoc_1985_v107_p154} ionization
energies, \cite{Watson_ChemPhysLett_2013_v555_p235} proton affinities and
gas-phase basicities \cite{Range_PhysChemChemPhys_2005_v7_p3070}, and much more.
\cite{Hehre_Ab_initio_MO_Theory_Book_1986}

These calculations are becoming routine as more and more experimental studies
employ some form of calculation to help interpret results or strengthen
conclusions. Despite all their successes and the rapid increase of computational
power over recent years, however, the computational demands of QM methods remain
prohibitively high for systems with more than 100 -- 200 atoms. Furthermore for
researchers interested in these large systems, calculations on a single
arrangement of atomic nuclei becomes increasingly insufficient to quantify the
behavior of those systems.

For such applications, we turn our attention back to statistical mechanics with
the aim of ultimately applying those principles to molecular mechanical
simulations of large molecules that often contain thousands---even hundreds of
thousands---of atoms.

\subsection{Statistical Mechanics}

Macroscopic chemical systems are composed of a vast number of atoms---on the
order of Avogadro's number, or $6.022 \times 10 ^ {23}$. How, then, can our
calculations of a single molecule or a small cluster of molecules be used to
predict the behavior of a collection of $10 ^ {23}$ molecules? For that we turn
to statistical mechanics and the idea of an \emph{ensemble}.

In a system with $N$ particles (where $N$ is typically on the order of
Avagadro's number in magnitude), there are $6 N$ total degrees of freedom in the
system corresponding to the position and momentum of each particle in all three
dimensions. This ultra-high, $6N$-dimensional space is referred to as
\emph{phase space}, and the collection of all points that conform to a small set
of thermodynamic constraints---\eg constant volume or energy---represents an
ensemble. \cite{McQuarrie_Book_StatMech_1973} The connection between this
imaginary ensemble of systems and experimental measurements of real systems was
provided by Gibbs. The experimental value of any system in the lab is postulated
to be equal to the value of that mechanical observable averaged over every
member of the ensemble. \cite{McQuarrie_Book_StatMech_1973} By knowing the
probability of finding a member of an ensemble with a given set of properties,
this ensemble average can be calculated according to Eq. \ref{eq1:EnsembleAvg}.

\begin{align}
   \left < A \right > & = \frac {\sum_a W(a) A(a)} {\sum_a W(a)} \nonumber \\
                      & = \sum_a P(a) A(a)
   \label{eq1:EnsembleAvg}
\end{align}
where $W(a)$ can be thought of as the number of states in the ensemble with the
same value for the property of $A$. $P(a)$ is the normalized probability for
that state, where $\sum_a W(a)$ is the normalization factor.

Given that there are on the order of $10^{23}$ particles in the typical system,
the number of ensemble members of which the average must be taken appears at
first glance to be intractable. However, it turns out that the mean square
fluctuations of measurable properties within the ensemble scale as roughly $1 /
\sqrt{N}$ where $N$ is the number of particles in the system.  Because $N$ is on
the order of $10 ^ {23}$, the fluctuations around the most probable value in the
ensemble vanish and the ensemble average and most probable value become
identical. The problem of calculating the ensemble average of a desired property
is reduced to the far more tractable task of calculating its most probable
value.

The most commonly used ensemble, called the \emph{canonical} ensemble, is
constrained such that each member has the same particle count, volume, and
temperature (NVT). Other common ensembles are the microcanonical ensemble (NVE),
the grand canonical ensemble ($\mu$VT), and the isobaric-isothermal ensemble
(NpT), where E, $\mu$, and p stand for constant energy, chemical potential, and
pressure, respectively. At typical temperatures and particle densities, the
fluctuations of mechanical properties in each of these ensembles becomes
negligible. Therefore, these ensembles are effectively equivalent to one
another, allowing us to choose the one that is most convenient to work with
mathematically.

The link between these ensembles and thermodynamics is the natural logarithm of
the \emph{partition function}, which happens to be the normalization constant
from Eq. \ref{eq1:EnsembleAvg} for each of the ensembles. The partition
functions of the common ensembles are shown below in Eqs.
\ref{eq1:MicrocanonicalPF} to \ref{eq1:IsobaricIsothermPF}.

\begin{align}
   \text{Microcanonical} && \Omega(N, V, E) & = \omega(E) 
      \label{eq1:MicrocanonicalPF} \\
   \text{Canonical} && Q(N, V, T) & = \sum_E \Omega(N, V, E) 
               \exp(-\beta E) 
               \label{eq1:CanonicalPF} \\
   \text{Grand Canonical} && \Xi(\mu, V, T) & = \sum _ N 
               Q(N, V, T) \exp(\beta \mu N) 
               \label{eq1:GrandCanonicalPF} \\
   \text{Isobaric-Isothermal} && \Delta(N, p, T) & = \sum _ V
               Q(N, V, T) \exp(-\beta p V)
               \label{eq1:IsobaricIsothermPF}
\end{align}
where $\omega$ is the total number of states with a given energy and $\beta$ is
$1 / k_BT$. According to the principle of equal \emph{a priori} probabilities,
all states in the microcanonical ensemble are equally probable for the simple
reason that there is no reason to assume otherwise.

The logarithm of the partition function for each ensemble is directly
proportional to the thermodynamic function that has the same set of `natural'
variables. These connections are summarized below in Eqs. \ref{eq1:ThermoLink1}
to \ref{eq1:ThermoLink4}. \cite{McQuarrie_Book_StatMech_1973}

\begin{align}
   \text{Microcanonical} && S & = k \ln \left ( \Omega (N, V, E) \right ) 
      \label{eq1:ThermoLink1} \\
   \text{Canonical} && A & = -kT \ln \left ( Q (N, V, T) \right ) 
      \label{eq1:ThermoLink2} \\
   \text{Grand Canonical} && p V & = kT \ln \left ( \Xi (\mu, V, T) \right ) 
      \label{eq1:ThermoLink3} \\
   \text{Isobaric Isothermal} && G & = - kT \ln \left ( \Delta (N, p, T) \right )
      \label{eq1:ThermoLink4}
\end{align}

With the link to classical thermodynamics now firmly established through the
partition function, statistical mechanics can now explain the whole of
thermodynamics from the microscopic behavior of individual atoms and molecules.
One of the principle challenges of computational chemistry becomes how to
efficiently estimate the partition function.

Significant effort in computational chemistry centers on estimating the
canonical partition function $Q(N, V, T)$. The na\"ive approach to calculate the
sum in \ref{eq1:CanonicalPF} would be to calculate the degeneracy of each energy
level ($\Omega(N, V, E)$) and scale it with the exponential weighting factor
($\exp(-\beta E)$) called the \emph{Boltzmann factor}. Due to the immeasurable
size of $\Omega(N, V, E)$, however, this approach is highly inefficient in
practice. It turns out that most of the effort put into computing $Q(N, V, T)$
result in terms that contribute very little to the partition function.

The next sections will discuss the various methods used to efficiently
approximate partition functions. Because it is infeasible to calculate the full
sums in Eqs. \ref{eq1:MicrocanonicalPF} to \ref{eq1:IsobaricIsothermPF}, they
are estimated using a representative subsample of the available points to
construct the needed distributions. The strategies of generating these
subsamples are collectively referred to as \emph{sampling}. The two most common
approaches---Monte Carlo and Molecular Dynamics---are discussed in the following
sections.

\subsubsection{Monte Carlo}

One approach to approximating Eq. \ref{eq1:CanonicalPF}, called \emph{Monte
Carlo} (MC) sampling, is to select new configurations of atomic positions at
random in the molecular system, evaluate the energy of that structure, and add
it to the sum of the partition function. This is equivalent to reorganizing the
sum in Eq. \ref{eq1:CanonicalPF} to sum over individual states rather than
energy levels. Eq. \ref{eq1:CanonicalPF} is then estimated as
\begin{equation}
   Q(N, V, T) \approx \sum_{i=1}^{N_{samples}} \exp(-\beta E_i)
   \label{eq1:MonteCarloCanonicalPF}
\end{equation}

Because we assume no prior knowledge of phase space beforehand, using random
configurations in MC sampling is critical to avoid introducing bias into the
subsample. The MC approach to approximating the partition function proves to be
highly inefficient. Most random atom configurations in a chemical system
correspond to species that are chemically ridiculous and contribute $\sim 0$ to
the partition function.

For example, consider characterizing the phase space of an ethane molecule using
MC. A random configuration is generated by placing both carbon atoms and all six
hydrogen atoms at a random point in space, evaluating the energy of that
configuration using a QM calculation, and adding that term to the summation in
Eq. \ref{eq1:MonteCarloCanonicalPF}. Figure \ref{fig:EthaneMC} depicts two
conformations of ethane with an equal probability of being chosen and added to
the summation in Eq. \ref{eq1:MonteCarloCanonicalPF}. Of the two conformations,
only one will have an energy low enough to contribute significantly to the $Q(N,
V, T)$. It should be easy to see that there are far more chemically absurd
arrangements of the atoms in ethane than chemically reasonable ones (try
throwing eight darts at a dartboard while blindfolded and see how often a
configuration resembles an ethane molecule).

\begin{figure}
   \includegraphics[width=6.5in]{EthaneMC.ps}
   \caption{Two conformations of a molecule of ethane. The conformation on the
            left is the typical `staggered' conformation known to be the
            lowest-energy structure. The structure on the right is an absurd
            conformation that is never found experimentally. While the structure
            on the right contributes negligibly to the partition function, it is
            an equally likely structure to be proposed by Monte Carlo as the one
            on the left.}
   \label{fig:EthaneMC}
\end{figure}

While MC suffers severe limitations,
\citeauthor{Metropolis_JChemPhys_1953_v21_p1087} proposed a modification to the
traditional MC approach that helped alleviate many of the problems described
above. \cite{Metropolis_JChemPhys_1953_v21_p1087} This variant, described below,
is called \emph{Metropolis Monte Carlo} after the method's architect.

\subsubsection*{Metropolis Monte Carlo}

Metropolis's great breakthrough in MC methods is a subtle change to the standard
approach. Instead of generating random structures and adding them all to the
ensemble with a weight equal to the Boltzmann factor, random structures are
generated and accepted as \emph{full} members of the ensemble with a probability
proportional to the Boltzmann factor. This way, lower-energy structures are more
likely to be added to the ensemble than higher-energy structures, since the
probability of accepting the former into the ensemble is significantly greater.

In practice, an ensemble is built using Metropolis MC by constructing a chain of
states beginning with some initial structure. The `next' structure is generated
randomly and accepted into the ensemble with a probability that ensures the
constructed ensemble reproduces the correct probability distributions for each
state. The process of generating a random conformation and evaluating its
acceptance into the ensemble is called a \emph{trial move}.

The resulting chain of states generated by Metropolis MC is called a
\emph{Markov chain}, and it has two important qualities. First, trial moves are
generated via a finite set of available, predetermined moves that cannot change
as the Markov chain grows. Second, a Markov chain is said to be
memoryless---that is, the probability of accepting a proposed structure depends
\emph{only} on the current state and not on any other state that has come
before. Because thermodynamics deals with chemical equilibria, an ensemble built
from a Markov chain of states needs an additional property---reversibility.

A reversible Markov chain needs to satisfy the additional constraint of
reversibility. A reversible Markov chain obeys \emph{detailed balance}, a
relationship shown in Eq. \ref{eq1:DetailedBalance}.
\begin{equation}
   p_i \pi_{i \rightarrow j} = p_j \pi_{i \rightarrow j}
   \label{eq1:DetailedBalance}
\end{equation}
where $p_i$ is the probability of being in state $i$ and $\pi_{i \rightarrow j}$
is the probability of accepting the proposed change of going to state $j$ from
state $i$ (called the \emph{transition probability}). The detailed balance
condition in a Markov chain asserts an equilibrium between all states in the
chain. Eq. \ref{eq1:DetailedBalance} is nothing more than a common equilibrium
expression encountered in general chemistry where $p_i$ is the `concentration'
of state $i$ in the Markov chain and the transition probability is the `rate' of
changing from state $i$ to state $j$.

The last remaining detail of Metropolis MC is to define a transition probability
equation that satisfies detailed balance. For the canonical ensemble, where the
probability of being in state $i$ is proportional to the Boltzmann factor, Eq.
\ref{eq1:MetropolisMC} satisfies detailed balance.
\begin{equation}
   \pi_{i \rightarrow j} = \min \left \lbrace 1, \frac {\exp ( -\beta E _ i )}
                         {\exp ( -\beta E _ j ) } \right \rbrace
   \label{eq1:MetropolisMC}
\end{equation}
Eq. \ref{eq1:MetropolisMC} can be inserted into Eq. \ref{eq1:DetailedBalance} to
verify that this choice for the transition probability satisfies detailed
balance and therefore results in a reversible Markov chain. Models using the
Metropolis MC approach instead of traditional MC are far more efficient---so
much so that Monte Carlo is often synonymous with Metropolis Monte Carlo,
\cite{Tuckerman_Book_StatMech_TheoryAndSim,Leach_Book_MolModel_2001} and that
convention will be adopted for the rest of this dissertation.

One concern that Metropolis MC does not address, however, is the propensity for
random choices to result in meaningless structures. This is alleviated by
starting from a chemically reasonable structure and limiting the magnitude of
the structure differences allowed in each trial move---a technique referred to
as \emph{importance sampling}. The step size becomes a tunable parameter of the
method. If it is too small, then it will take a long time to fill the ensemble
with different structures. However, if it is too large, the likelihood of
proposing reasonable structures will drop off and the acceptance rate will
suffer.

\subsection{Molecular Dynamics and the Ergodic Hypothesis}

An alternative method for constructing a statistical ensemble of states, called
\emph{molecular dynamics} (MD), corresponds to generating structures by
integrating the equations of motion for molecular systems and building ensembles
from the resulting trajectories. The idea that a time-average over a trajectory
is equal to an ensemble average is called the \emph{ergodic hypothesis}, and is
the cornerstone of the MD methods that have become some of the most useful tools
in computational studies of molecular systems.

The most common equations of motion used in MD simulations are those from
classical mechanics. The force on each atomic nucleus is calculated as the
gradient of the potential energy function $U(\vec{x})$ at the nuclear centers
and then integrated numerically according to Newton's laws. A discussion of
computational MD and numerical integration of the classical equations of motion
is presented in \ref{appendixA}.

\subsection{An Example}

\section{Molecular Mechanics}

We saw from Sec. \ref{sec:CompQuantumMech} that computational chemists use
quantum mechanics to solve the electronic Schr\"odinger equation in order to
calculate the energy as a function of nuclear coordinates. For small molecules
containing 20 -- 30 atoms, there are typically a small number of conformations
that the molecule can reasonably adopt at typical temperatures, and partition
functions can be reasonably approximated using a handful of different
structures.

As system sizes increase, however, it becomes increasingly difficult to use QM
methods for two reasons. First, the computational demand for obtaining the
energy of a single structure rapidly increases. Second, phase space becomes so
massive that calculating the potential energy of a small number of snapshots is
no longer a reasonable approximation to the partition function. Furthermore,
there is no 
