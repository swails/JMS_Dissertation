\chapter{INTRODUCTION}

\section{Origins of Computational Chemistry}
The seeds of computational chemistry were sown in the mid-1800s with Ludwig
Eduard Boltzmann's formulation of \textit{statistical mechanics}. In an era when
the existence of atoms and molecules was hotly disputed within the physics
community, Boltzmann fathered a theory in which the behavior and interaction of
individual atoms or molecules on the microscopic scale could be used to
describe and predict macroscopic phenomena. Using his theorems and equations,
it became possible to reduce the problem of simulating $\sim 10^{23}$ molecules
to simulating $\sim 1$ molecule. Boltzmann used this to great effect in
describing and deriving previously-known, phenomenological equations for ideal
gases, such as the widely known equation of state, $P V = n R T$. All that
remains to provide the foundation for using molecular simulations is the proper
description of atoms and molecules on the microscopic scale.

The theories required to accurately model the behavior of individual atoms
interacting with each other and their surroundings would not be developed until
the first half of the $20^{th}$ century with the advent of \textit{quantum
mechanics}. The limits of classical mechanics became apparent when considering
the Rayleigh-Jeans formula for calculating the spectral emission of a radiating
black body. The Rayleigh-Jeans law, given by Eq. \ref{eq:RayleighJeans} and
completely derived from the laws of classical mechanics, predicts infinite
emission at high frequencies---a clear violation of the well-established law of
conservation of energy that directly contradicts experimental observations.
\begin{equation}
   B_{\nu} (T)  = \frac{2 \nu^2 k T} {c^2}
   \label{eq:RayleighJeans}
\end{equation}
To address this apparent disparity, Max Planck suggested that the error in the
classical mechanical approach was to assume a continuous emission spectrum.
Instead, Planck suggested that the emission spectra was quantized, leading to an
equation that agreed much closer with experiment. This idea of quantized energy
emissions, while developed to reconcile the mathematics of black-body radiation
with experimental measurements, would forever change our understanding of the
microscopic world.

As quantum mechanics matured, our ability to explain and predict behavior at the
atomic scale dramatically improved. In 1929, Paul Dirac proclaimed, ``The
fundamental laws necessary for the mathematical treatment of a large part of
physics and the whole of chemistry are thus completely known, and the difficulty
lies only in the fact that the application of these laws leads to equations that
are too complex to be solved.'' Even approximations designed to simplify the
equations of quantum mechanics in molecular systems resulted in computations too
complex to apply to all but the simplest systems. With the fundamental theory
necessary to describe single molecules and the machinery required to extend
that description to experimental measurements now established, computers
provided the catalyst that thrust theoretical chemistry into a prominent role in
the field.

The next sections will describe the theory of quantum mechanics and the
approximations typically employed to simplify its equations, followed by
a description of statistical mechanics.

\subsection{Quantum Mechanics}

Twenty years after Planck introduced the idea of quantized oscillators to
explain black-body radiation, Erwin Schr\"odinger introduced a wave equation
formulation of quantum mechanics (QM). \cite{Schrodinger1926} Schr\"odinger's
equation (Eq. \ref{eq:SchrodingerEquation}) bears a strong resemblance to
Hamilton's formulation of classical mechanics by employing an analogous
\textit{Hamiltonian} operator comprised of a kinetic energy term (related to the
momentum operator) and a potential energy term.
\begin{align}
   E \Psi(\vec{x}, t) & = \hat{H} \Psi(\vec{x}) \nonumber \\
   & = \left ( -\frac {\hbar ^ 2} {2 m} \bigtriangledown ^ 2 + V(\vec{x})
   \right) \Psi(\vec{x})
   \label{eq:SchrodingerEquation}
\end{align}
where $E$ is the total energy, $\hat{H}$ is the Hamiltonian operator, and
$\Psi(\vec{x}, t)$ is the wavefunction---the central object of Schr\"odinger's
equation containing all of the information and properties inherent to the
system.

Equation \ref{eq:SchrodingerEquation} is a special form of Schr\"odinger's
equation corresponding to a stationary state (\ie the potential function is
time-independent, so the energy for that state is constant). In chemistry when
we wish to calculate observable properties of a system composed of atoms, the
kinetic energy is the sum of the kinetic energies of the atomic particles in the
system, and the potential energy is calculated as the interaction of all charged
particles---protons and electrons---in the electric field they create.

The wavefunction contains all of the information about each of the particles in
the system. As the number of particles in the system increases, so too does the
complexity of wavefunction and the effort required to solve Eq.
\ref{eq:SchrodingerEquation}. Therefore, we turn to a number of approximations
developed to simplify computing a solution to Schr\"odinger's equation.

\subsubsection{Born-Oppenheimer Approximation}

The Born-Oppenheimer approximation (BOA) is almost ubiquitous in the field.
Electrons can move far more rapidly than nucleons since electrons are roughly
\mbox{1 000} times lighter. This implies that electrons can reorganize around
moving nuclei so quickly that nuclear protons are always subject to the
potential from the average electric field generated by the electrons.

Using the BOA, the wavefunction of a molecular system can be separated into two
parts: an electronic part where the nuclei are treated as fixed point charges,
and a nuclear part where the electrons are treated as a mean field.
\cite{McQuarrie_Book_PhysChem_1997} So critical is the BOA to computational
chemistry that it appears at the heart of nearly every aspect of the field.

\subsubsection{Computational Quantum Mechanics}
\label{sec:CompQuantumMech}

The main goal of most QM calculations in chemistry and molecular physics is to
determine atomic and molecular properties of the system by estimating the
electronic part of the wavefunction from the BOA. These calculations have
provided valuable assistance to experimental investigations. QM calculations
can provide reliable measurements of molecular geometries,
\cite{Jeletic_JOrganometChem_2011_v696_p3127} potential and free energy barriers
of chemical reactions, \cite{Chandrasekhar_JAmChemSoc_1985_v107_p154} ionization
energies, \cite{Watson_ChemPhysLett_2013_v555_p235} proton affinities and
gas-phase basicities \cite{Range_PhysChemChemPhys_2005_v7_p3070}, and much more.
\cite{Hehre_Ab_initio_MO_Theory_Book_1986}

These calculations are becoming routine as more and more experimental studies
employ some form of calculation to help interpret results or strengthen
conclusions. Despite all their successes and the rapid increase of computational
power over recent years, however, the computational demands of QM methods remain
prohibitively high for systems with more than 100 -- 200 atoms. Furthermore for
researchers interested in these large systems, calculations on a single
arrangement of atomic nuclei becomes increasingly insufficient to quantify the
behavior of those systems.

For such applications, we turn our attention back to statistical mechanics with
the aim of ultimately applying those principles to molecular mechanical
simulations of large molecules that often contain thousands---even hundreds of
thousands---of atoms.

\subsection{Statistical Mechanics}

Macroscopic chemical systems are composed of a vast number of atoms---on the
order of Avogadro's number, or $6.022 \times 10 ^ {23}$. How, then, can our
calculations of a single molecule or a small cluster of molecules be used to
predict the behavior of a collection of $10 ^ {23}$ molecules? For that we turn
to statistical mechanics and the idea of an \emph{ensemble}.

In a system with $N$ particles (where $N$ is typically on the order of
Avagadro's number in magnitude), there are $6 N$ total degrees of freedom in the
system corresponding to the position and momentum of each particle in all three
dimensions. This ultra-high, $6N$-dimensional space is referred to as
\emph{phase space}, and the collection of all points that conform to a small set
of thermodynamic constraints---\eg constant volume or energy---represents an
ensemble. \cite{McQuarrie_Book_StatMech_1973} The connection between this
imaginary ensemble of systems and experimental measurements of real systems was
provided by Gibbs. The experimental value of any system in the lab is postulated
to be equal to the value of that mechanical observable averaged over every
member of the ensemble. \cite{McQuarrie_Book_StatMech_1973} By knowing the
probability of finding a member of an ensemble with a given set of properties,
this ensemble average can be calculated according to Eq. \ref{eq:EnsembleAvg}.

\begin{align}
   \left < A \right > & = \frac {\sum_a W(a) A(a)} {\sum_a W(a)} \nonumber \\
                      & = \sum_a P(a) A(a)
   \label{eq:EnsembleAvg}
\end{align}
where $W(a)$ can be thought of as the number of states in the ensemble with the
same value for the property of $A$. $P(a)$ is the normalized probability for
that state, where $\sum_a W(a)$ is the normalization factor.

Given that there are on the order of $10^{23}$ particles in the typical system,
the number of ensemble members of which the average must be taken appears at
first glance to be intractable. However, it turns out that the mean square
fluctuations of measurable properties within the ensemble scale as roughly $1 /
\sqrt{N}$ where $N$ is the number of particles in the system.  Because $N$ is on
the order of $10 ^ {23}$, the fluctuations around the most probable value in the
ensemble vanish and the ensemble average and most probable value become
identical. The problem of calculating the ensemble average of a desired property
is reduced to the far more tractable task of calculating its most probable
value.

The most commonly used ensemble, called the \emph{canonical} ensemble, is
constrained such that each member has the same particle count, volume, and
temperature (NVT). Other common ensembles are the microcanonical ensemble (NVE),
the grand canonical ensemble ($\mu$VT), and the isobaric-isothermal ensemble
(NpT), where E, $\mu$, and p stand for constant energy, chemical potential, and
pressure, respectively. At typical temperatures and particle densities, the
fluctuations of mechanical properties in each of these ensembles becomes
negligible. Therefore, these ensembles are effectively equivalent to one
another, allowing us to choose the one that is most convenient to work with
mathematically.

The link between these ensembles and thermodynamics is the natural logarithm of
the \emph{partition function}, which happens to be the normalization constant
from Eq. \ref{eq:EnsembleAvg} for each of the ensembles. The partition functions
of the common ensembles are shown below in Eqs. \ref{eq:MicrocanonicalPF} to
\ref{eq:IsobaricIsothermPF}.

\begin{align}
   \text{Microcanonical} && \Omega(N, V, E) & = \omega(E) 
      \label{eq:MicrocanonicalPF} \\
   \text{Canonical} && Q(N, V, T) & = \sum_E \Omega(N, V, E) 
               \exp(-\beta E) 
               \label{eq:CanonicalPF} \\
   \text{Grand Canonical} && \Xi(\mu, V, T) & = \sum _ N 
               Q(N, V, T) \exp(\beta \mu N) 
               \label{eq:GrandCanonicalPF} \\
   \text{Isobaric-Isothermal} && \Delta(N, p, T) & = \sum _ V
               Q(N, V, T) \exp(-\beta p V)
               \label{eq:IsobaricIsothermPF}
\end{align}
where $\omega$ is the total number of states with a given energy and $\beta$ is
$1 / k_BT$. According to the principle of equal \emph{a priori} probabilities,
all states in the microcanonical ensemble are equally probable for the simple
reason that there is no reason to assume otherwise.

The logarithm of the partition function for each ensemble is directly
proportional to the thermodynamic function that has the same set of `natural'
variables. These connections are summarized below in Eqs. \ref{eq:ThermoLink1}
to \ref{eq:ThermoLink4}. \cite{McQuarrie_Book_StatMech_1973}

\begin{align}
   \text{Microcanonical} && S & = k \ln \left ( \Omega (N, V, E) \right ) 
      \label{eq:ThermoLink1} \\
   \text{Canonical} && A & = -kT \ln \left ( Q (N, V, T) \right ) 
      \label{eq:ThermoLink2} \\
   \text{Grand Canonical} && p V & = kT \ln \left ( \Xi (\mu, V, T) \right ) 
      \label{eq:ThermoLink3} \\
   \text{Isobaric Isothermal} && G & = - kT \ln \left ( \Delta (N, p, T) \right )
      \label{eq:ThermoLink4}
\end{align}

With the link to classical thermodynamics now firmly established through the
partition function, statistical mechanics can now explain the whole of
thermodynamics from the microscopic behavior of individual atoms and molecules.
One of the principle challenges of computational chemistry becomes how to
efficiently estimate the partition function.

Significant effort in computational chemistry centers on estimating the
canonical partition function $Q(N, V, T)$. The na\"ive approach to calculate the
sum in \ref{eq:CanonicalPF} would be to calculate the degeneracy of each energy
level ($\Omega(N, V, E)$) and scale it with the exponential weighting factor
($\exp(-\beta E)$) called the \emph{Boltzmann factor}. Due to the immeasurable
size of $\Omega(N, V, E)$, however, this approach is highly inefficient in
practice. It turns out that most of the effort put into computing $Q(N, V, T)$
result in terms that contribute very little to the partition function.

The next sections will discuss the various methods used to efficiently
approximate partition functions. Because it is infeasible to calculate the full
sums in Eqs. \ref{eq:MicrocanonicalPF} to \ref{IsobaricIsothermPF}, they are
estimated using a representative subsample of the available points to construct
the needed distributions. The strategies of generating these subsamples are
collectively referred to as \emph{sampling}. The two most common
approaches---Monte Carlo and Molecular Dynamics---are discussed in the following
sections.

\subsubsection{Monte Carlo}

One approach to approximating Eq. \ref{eq:CanonicalPF}, called \emph{Monte
Carlo} sampling, is to select new configurations of atomic positions at random
in the molecular system, evaluate the energy of that structure, and add it to
the sum of the partition function. This is equivalent to reorganizing the sum in
Eq. \ref{eq:CanonicalPF} to sum over individual states rather than energy
levels. Eq. \ref{eq:CanonicalPF} is then estimated as
\begin{equation}
   Q(N, V, T) \approx \sum_{i=1}^{N_{samples}} \exp(-\beta E_i)
   \label{eq:MonteCarloCanonicalPF}
\end{equation}

Because we assume no prior knowledge of phase space beforehand, using random
configurations in Monte Carlo sampling is critical to avoid introducing bias
into the subsample. The Monte Carlo approach to approximating the partition
function proves to be highly inefficient. Most random atom configurations in a
chemical system correspond to species that are chemically ridiculous and
contribute $\sim 0$ to the partition function.

For example, consider characterizing the phase space of an ethane molecule using
Monte Carlo. A random configuration is generated by placing both carbon atoms
and all six hydrogen atoms at a random point in space, evaluating the energy of
that configuration using a QM calculation, and adding that term to the summation
in Eq. \ref{eq:MonteCarloCanonicalPF}. Figure \ref{fig:EthaneMC} depicts two
conformations of ethane with an equal probability of being chosen and added to
the summation in Eq. \ref{eq:MonteCarloCanonicalPF}. Of the two conformations,
only one will have an energy low enough to contribute significantly to the $Q(N,
V, T)$. It should be easy to see that there are far more chemically absurd
arrangements of the atoms in ethane than chemically reasonable ones.

\begin{figure}
   \includegraphics[width=6.5in]{EthaneMC.eps}
   \caption{Two conformations of a molecule of ethane. The conformation on the
            right is the typical `staggered' conformation known to be the
            lowest-energy structure. The structure on the right is an absurd
            conformation that is never found experimentally. While the structure
            on the right contributes negligibly to the partition function, it is
            an equally likely structure to be proposed by Monte Carlo as the one
            on the left.}
   \label{fig:EthaneMC}
\end{figure}

While Monte Carlo suffers severe limitations,
\citeauthor{Metropolis_JChemPhys_1953_v21_p1087} proposed a modification to the
traditional Monte Carlo approach that helped alleviate many of the problems
described above. \cite{Metropolis_JChemPhys_1953_v21_p1087} This variant,
described below, is called \emph{Metropolis Monte Carlo} after the method's
architect.

\subsubsection*{Metropolis Monte Carlo}

\subsection{The Ergodic Hypothesis and Molecular Dynamics}

\section{Molecular Mechanics}

We saw from Sec. \ref{sec:CompQuantumMech} that computational chemists use
quantum mechanics to solve the electronic Schr\"odinger equation in order to
calculate the energy as a function of nuclear coordinates. For small molecules
containing 20 -- 30 atoms, there are typically a small number of conformations
that the molecule can reasonably adopt at typical temperatures, and partition
functions can be reasonably approximated using a handful of different
structures.

As system sizes increase, however, it becomes increasingly difficult to use QM
methods for two reasons. First, the computational demand for obtaining the
energy of a single structure rapidly increases. Second, phase space becomes so
massive that calculating the potential energy of a small number of snapshots is
no longer a reasonable approximation to the partition function. Furthermore,
there is no 
